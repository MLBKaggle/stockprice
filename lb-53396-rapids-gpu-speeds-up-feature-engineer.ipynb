{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Your upvoting👍 is important to me and can grealy encourage me!🥰🥰🥰","metadata":{}},{"cell_type":"code","source":"import gc  # Garbage collection for memory management\nimport os  # Operating system-related functions\nimport time  # Time-related functions\nimport warnings  # Handling warnings\nfrom itertools import combinations  # For creating combinations of elements\nfrom warnings import simplefilter  # Simplifying warning handling\n\n# 📦 Importing machine learning libraries\nimport joblib  # For saving and loading models\nimport lightgbm as lgb  # LightGBM gradient boosting framework\nimport numpy as np  # Numerical operations\nimport pandas as pd  # Data manipulation and analysis\nfrom sklearn.metrics import mean_absolute_error  # Metric for evaluation\nfrom sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n\n# 🤐 Disable warnings to keep the code clean\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n\n# 📊 Define flags and variables\nis_offline = False  # Flag for online/offline mode\nis_train = True  # Flag for training mode\nis_infer = True  # Flag for inference mode\nmax_lookback = np.nan  # Maximum lookback (not specified)\nsplit_day = 435  # Split day for time series data\n","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":4.666556,"end_time":"2023-10-31T03:48:34.580575","exception":false,"start_time":"2023-10-31T03:48:29.914019","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T05:06:23.517974Z","iopub.execute_input":"2023-11-07T05:06:23.518263Z","iopub.status.idle":"2023-11-07T05:06:28.331305Z","shell.execute_reply.started":"2023-11-07T05:06:23.518237Z","shell.execute_reply":"2023-11-07T05:06:28.330236Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# 📂 Read the dataset from a CSV file using Pandas\ndf = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n\n# 🧹 Remove rows with missing values in the \"target\" column\ndf = df.dropna(subset=[\"target\"])\n\n# 🔁 Reset the index of the DataFrame and apply the changes in place\ndf.reset_index(drop=True, inplace=True)\n\n# 📏 Get the shape of the DataFrame (number of rows and columns)\ndf_shape = df.shape\n","metadata":{"papermill":{"duration":18.0321,"end_time":"2023-10-31T03:48:52.617699","exception":false,"start_time":"2023-10-31T03:48:34.585599","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T05:06:28.333057Z","iopub.execute_input":"2023-11-07T05:06:28.333730Z","iopub.status.idle":"2023-11-07T05:06:45.993995Z","shell.execute_reply.started":"2023-11-07T05:06:28.333696Z","shell.execute_reply":"2023-11-07T05:06:45.992970Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# 🧹 Function to reduce memory usage of a Pandas DataFrame\ndef reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n    \n    # 📏 Calculate the initial memory usage of the DataFrame\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    # 🔄 Iterate through each column in the DataFrame\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        # Check if the column's data type is not 'object' (i.e., numeric)\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            # Check if the column's data type is an integer\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # Check if the column's data type is a float\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n\n    # ℹ️ Provide memory optimization information if 'verbose' is True\n    if verbose:\n        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"Decreased by {decrease:.2f}%\")\n\n    # 🔄 Return the DataFrame with optimized memory usage\n    return df\n","metadata":{"papermill":{"duration":0.122113,"end_time":"2023-10-31T03:48:52.755627","exception":false,"start_time":"2023-10-31T03:48:52.633514","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T05:06:45.996753Z","iopub.execute_input":"2023-11-07T05:06:45.997474Z","iopub.status.idle":"2023-11-07T05:06:46.122048Z","shell.execute_reply.started":"2023-11-07T05:06:45.997436Z","shell.execute_reply":"2023-11-07T05:06:46.121036Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# 🏎️ Import Numba for just-in-time (JIT) compilation and parallel processing\nfrom numba import njit, prange\n\n# 📊 Function to compute triplet imbalance in parallel using Numba\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    # 🔁 Loop through all combinations of triplets\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        \n        # 🔁 Loop through rows of the DataFrame\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n            \n            # 🚫 Prevent division by zero\n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n\n    return imbalance_features\n\n# 📈 Function to calculate triplet imbalance for given price data and a DataFrame\ndef calculate_triplet_imbalance_numba(price, df):\n    # Convert DataFrame to numpy array for Numba compatibility\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    # Calculate the triplet imbalance using the Numba-optimized function\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    # Create a DataFrame from the results\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features\n","metadata":{"papermill":{"duration":0.616324,"end_time":"2023-10-31T03:48:53.387757","exception":false,"start_time":"2023-10-31T03:48:52.771433","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T05:06:46.123419Z","iopub.execute_input":"2023-11-07T05:06:46.123711Z","iopub.status.idle":"2023-11-07T05:06:46.693933Z","shell.execute_reply.started":"2023-11-07T05:06:46.123685Z","shell.execute_reply":"2023-11-07T05:06:46.693061Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"#### We use CUDF speed up pandas in `imbalance_features` func.","metadata":{}},{"cell_type":"code","source":"# 📊 Function to generate imbalance features\ndef imbalance_features(df):\n    import cudf\n    df = cudf.from_pandas(df)\n    \n    # Define lists of price and size-related column names\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n    # V1 features\n    # Calculate various features using Pandas eval function\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"ask_price + bid_price\")/2\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"imbalance_size-matched_size\")/df.eval(\"matched_size+imbalance_size\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    \n    # Create features for pairwise price imbalances\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n        \n    # V2 features\n    # Calculate additional features\n    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    \n    # Calculate various statistical aggregation features\n    \n        \n    # V3 features\n    # Calculate shifted and return features for specific columns\n    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n        for window in [1, 2, 3, 10]:\n            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n    \n    # Calculate diff features for specific columns\n    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size']:\n        for window in [1, 2, 3, 10]:\n            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n    df = df.to_pandas()\n    # Replace infinite values with 0\n    return df.replace([np.inf, -np.inf], 0)\n\ndef numba_imb_features(df):\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n    \n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n        \n    # Calculate triplet imbalance features using the Numba-optimized function\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n    return df\n\n# 📅 Function to generate time and stock-related features\ndef other_features(df):\n    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n\n    # Map global features to the DataFrame\n    for key, value in global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n\n    return df\n\n# 🚀 Function to generate all features by combining imbalance and other features\ndef generate_all_features(df):\n    # Select relevant columns for feature generation\n    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n    df = df[cols]\n    \n    # Generate imbalance features\n    df = imbalance_features(df)\n    df = numba_imb_features(df)\n    # Generate time and stock-related features\n    df = other_features(df)\n    gc.collect()  # Perform garbage collection to free up memory\n    \n    # Select and return the generated features\n    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n    \n    return df[feature_name]\n","metadata":{"papermill":{"duration":0.024762,"end_time":"2023-10-31T03:48:53.427261","exception":false,"start_time":"2023-10-31T03:48:53.402499","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T05:06:46.695206Z","iopub.execute_input":"2023-11-07T05:06:46.695505Z","iopub.status.idle":"2023-11-07T05:06:46.714420Z","shell.execute_reply.started":"2023-11-07T05:06:46.695479Z","shell.execute_reply":"2023-11-07T05:06:46.713545Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"weights = [\n    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n]\n\nweights = {int(k):v for k,v in enumerate(weights)}","metadata":{"papermill":{"duration":0.019095,"end_time":"2023-10-31T03:48:53.451201","exception":false,"start_time":"2023-10-31T03:48:53.432106","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T05:06:46.715697Z","iopub.execute_input":"2023-11-07T05:06:46.716032Z","iopub.status.idle":"2023-11-07T05:06:46.728300Z","shell.execute_reply.started":"2023-11-07T05:06:46.716002Z","shell.execute_reply":"2023-11-07T05:06:46.727503Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Check if the code is running in offline or online mode\nif is_offline:\n    # In offline mode, split the data into training and validation sets based on the split_day\n    df_train = df[df[\"date_id\"] <= split_day]\n    df_valid = df[df[\"date_id\"] > split_day]\n    \n    # Display a message indicating offline mode and the shapes of the training and validation sets\n    print(\"Offline mode\")\n    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\nelse:\n    # In online mode, use the entire dataset for training\n    df_train = df\n    \n    # Display a message indicating online mode\n    print(\"Online mode\")\n","metadata":{"papermill":{"duration":0.013205,"end_time":"2023-10-31T03:48:53.469231","exception":false,"start_time":"2023-10-31T03:48:53.456026","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T05:06:46.729340Z","iopub.execute_input":"2023-11-07T05:06:46.729607Z","iopub.status.idle":"2023-11-07T05:06:46.739993Z","shell.execute_reply.started":"2023-11-07T05:06:46.729585Z","shell.execute_reply":"2023-11-07T05:06:46.739040Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Online mode\n","output_type":"stream"}]},{"cell_type":"code","source":"if is_train:\n    global_stock_id_feats = {\n        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n    }\n    if is_offline:\n        df_train_feats = generate_all_features(df_train)\n        print(\"Build Train Feats Finished.\")\n        df_valid_feats = generate_all_features(df_valid)\n        print(\"Build Valid Feats Finished.\")\n        df_valid_feats = reduce_mem_usage(df_valid_feats)\n    else:\n        df_train_feats = generate_all_features(df_train)\n        print(\"Build Online Train Feats Finished.\")\n\n    df_train_feats = reduce_mem_usage(df_train_feats)\n","metadata":{"papermill":{"duration":58.653136,"end_time":"2023-10-31T03:49:52.127357","exception":false,"start_time":"2023-10-31T03:48:53.474221","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T05:06:46.742327Z","iopub.execute_input":"2023-11-07T05:06:46.743061Z","iopub.status.idle":"2023-11-07T05:07:39.230118Z","shell.execute_reply.started":"2023-11-07T05:06:46.743028Z","shell.execute_reply":"2023-11-07T05:07:39.229068Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Build Online Train Feats Finished.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error\nimport gc\n\n# Assuming df_train_feats and df_train are already defined and df_train contains the 'date_id' column\n\n# Set up parameters for LightGBM\n\nlgb_params = {\n    \"objective\": \"mae\",\n    \"n_estimators\": 5000,\n    \"num_leaves\": 256,\n    \"subsample\": 0.6,\n    'learning_rate': 0.00871, \n    'max_depth': 11,\n    \"colsample_bytree\" : 0.8,\n    \"n_jobs\": 4,\n    \"device\": \"gpu\",\n    \"verbosity\": -1,\n    \"importance_type\": \"gain\",\n}\nfeature_name = list(df_train_feats.columns)\nprint(f\"Feature length = {len(feature_name)}\")\n\n# The total number of date_ids is 480, we split them into 5 folds with a gap of 5 days in between\nnum_folds = 5\nfold_size = 480 // num_folds\ngap = 5\n\nmodels = []\nscores = []\n\nmodel_save_path = 'modelitos_para_despues'  # Directory to save models\nif not os.path.exists(model_save_path):\n    os.makedirs(model_save_path)\n\n# We need to use the date_id from df_train to split the data\ndate_ids = df_train['date_id'].values\n\nfor i in range(num_folds):\n    start = i * fold_size\n    end = start + fold_size\n    \n    # Define the training and testing sets by date_id\n    if i < num_folds - 1:  # No need to purge after the last fold\n        purged_start = end - 2\n        purged_end = end + gap + 2\n        train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n    else:\n        train_indices = (date_ids >= start) & (date_ids < end)\n    \n    test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n    \n    df_fold_train = df_train_feats[train_indices]\n    df_fold_train_target = df_train['target'][train_indices]\n    df_fold_valid = df_train_feats[test_indices]\n    df_fold_valid_target = df_train['target'][test_indices]\n\n    print(f\"Fold {i+1} Model Training\")\n    \n    # Train a LightGBM model for the current fold\n    lgb_model = lgb.LGBMRegressor(**lgb_params)\n    lgb_model.fit(\n        df_fold_train[feature_name],\n        df_fold_train_target,\n        eval_set=[(df_fold_valid[feature_name], df_fold_valid_target)],\n        callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=100),\n            lgb.callback.log_evaluation(period=100),\n        ],\n    )\n\n    # Append the model to the list\n    models.append(lgb_model)\n    # Save the model to a file\n    model_filename = os.path.join(model_save_path, f'doblecito_{i+1}.txt')\n    lgb_model.booster_.save_model(model_filename)\n    print(f\"Model for fold {i+1} saved to {model_filename}\")\n\n    # Evaluate model performance on the validation set\n    fold_predictions = lgb_model.predict(df_fold_valid[feature_name])\n    fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n    scores.append(fold_score)\n    print(f\"Fold {i+1} MAE: {fold_score}\")\n\n    # Free up memory by deleting fold specific variables\n    del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n    gc.collect()\n\n# Calculate the average best iteration from all regular folds\naverage_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n\n# Update the lgb_params with the average best iteration\nfinal_model_params = lgb_params.copy()\nfinal_model_params['n_estimators'] = average_best_iteration\n\nprint(f\"Training final model with average best iteration: {average_best_iteration}\")\n\n# Train the final model on the entire dataset\nfinal_model = lgb.LGBMRegressor(**final_model_params)\nfinal_model.fit(\n    df_train_feats[feature_name],\n    df_train['target'],\n    callbacks=[\n        lgb.callback.log_evaluation(period=100),\n    ],\n)\n\n# Append the final model to the list of models\nmodels.append(final_model)\n\n# Save the final model to a file\nfinal_model_filename = os.path.join(model_save_path, 'doblez-conjunto.txt')\nfinal_model.booster_.save_model(final_model_filename)\nprint(f\"Final model saved to {final_model_filename}\")\n\n# Now 'models' holds the trained models for each fold and 'scores' holds the validation scores\nprint(f\"Average MAE across all folds: {np.mean(scores)}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-07T05:07:39.231890Z","iopub.execute_input":"2023-11-07T05:07:39.232267Z","iopub.status.idle":"2023-11-07T07:13:53.382072Z","shell.execute_reply.started":"2023-11-07T05:07:39.232234Z","shell.execute_reply":"2023-11-07T07:13:53.381098Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Feature length = 112\nFold 1 Model Training\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 7.10161\n[200]\tvalid_0's l1: 7.05668\n[300]\tvalid_0's l1: 7.03402\n[400]\tvalid_0's l1: 7.01657\n[500]\tvalid_0's l1: 7.00179\n[600]\tvalid_0's l1: 6.98942\n[700]\tvalid_0's l1: 6.97902\n[800]\tvalid_0's l1: 6.9697\n[900]\tvalid_0's l1: 6.96082\n[1000]\tvalid_0's l1: 6.95275\n[1100]\tvalid_0's l1: 6.94555\n[1200]\tvalid_0's l1: 6.93915\n[1300]\tvalid_0's l1: 6.9331\n[1400]\tvalid_0's l1: 6.92775\n[1500]\tvalid_0's l1: 6.92235\n[1600]\tvalid_0's l1: 6.91721\n[1700]\tvalid_0's l1: 6.91171\n[1800]\tvalid_0's l1: 6.90635\n[1900]\tvalid_0's l1: 6.90106\n[2000]\tvalid_0's l1: 6.89613\n[2100]\tvalid_0's l1: 6.89105\n[2200]\tvalid_0's l1: 6.88676\n[2300]\tvalid_0's l1: 6.88211\n[2400]\tvalid_0's l1: 6.87741\n[2500]\tvalid_0's l1: 6.87319\n[2600]\tvalid_0's l1: 6.8692\n[2700]\tvalid_0's l1: 6.86533\n[2800]\tvalid_0's l1: 6.86167\n[2900]\tvalid_0's l1: 6.85788\n[3000]\tvalid_0's l1: 6.85381\n[3100]\tvalid_0's l1: 6.85035\n[3200]\tvalid_0's l1: 6.8468\n[3300]\tvalid_0's l1: 6.84348\n[3400]\tvalid_0's l1: 6.84001\n[3500]\tvalid_0's l1: 6.83591\n[3600]\tvalid_0's l1: 6.83161\n[3700]\tvalid_0's l1: 6.82813\n[3800]\tvalid_0's l1: 6.82426\n[3900]\tvalid_0's l1: 6.82118\n[4000]\tvalid_0's l1: 6.81793\n[4100]\tvalid_0's l1: 6.81461\n[4200]\tvalid_0's l1: 6.81177\n[4300]\tvalid_0's l1: 6.80881\n[4400]\tvalid_0's l1: 6.806\n[4500]\tvalid_0's l1: 6.80268\n[4600]\tvalid_0's l1: 6.79833\n[4700]\tvalid_0's l1: 6.7939\n[4800]\tvalid_0's l1: 6.79035\n[4900]\tvalid_0's l1: 6.78682\n[5000]\tvalid_0's l1: 6.78314\nDid not meet early stopping. Best iteration is:\n[5000]\tvalid_0's l1: 6.78314\nModel for fold 1 saved to modelitos_para_despues/doblecito_1.txt\nFold 1 MAE: 6.783136727917537\nFold 2 Model Training\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 6.42456\n[200]\tvalid_0's l1: 6.39392\n[300]\tvalid_0's l1: 6.37689\n[400]\tvalid_0's l1: 6.36153\n[500]\tvalid_0's l1: 6.34825\n[600]\tvalid_0's l1: 6.33702\n[700]\tvalid_0's l1: 6.32685\n[800]\tvalid_0's l1: 6.31786\n[900]\tvalid_0's l1: 6.30967\n[1000]\tvalid_0's l1: 6.30198\n[1100]\tvalid_0's l1: 6.29534\n[1200]\tvalid_0's l1: 6.28931\n[1300]\tvalid_0's l1: 6.28389\n[1400]\tvalid_0's l1: 6.27862\n[1500]\tvalid_0's l1: 6.27374\n[1600]\tvalid_0's l1: 6.26842\n[1700]\tvalid_0's l1: 6.2631\n[1800]\tvalid_0's l1: 6.25813\n[1900]\tvalid_0's l1: 6.25347\n[2000]\tvalid_0's l1: 6.24849\n[2100]\tvalid_0's l1: 6.24335\n[2200]\tvalid_0's l1: 6.23864\n[2300]\tvalid_0's l1: 6.23437\n[2400]\tvalid_0's l1: 6.2293\n[2500]\tvalid_0's l1: 6.22475\n[2600]\tvalid_0's l1: 6.22029\n[2700]\tvalid_0's l1: 6.21644\n[2800]\tvalid_0's l1: 6.21296\n[2900]\tvalid_0's l1: 6.20961\n[3000]\tvalid_0's l1: 6.20622\n[3100]\tvalid_0's l1: 6.2028\n[3200]\tvalid_0's l1: 6.19937\n[3300]\tvalid_0's l1: 6.19593\n[3400]\tvalid_0's l1: 6.19281\n[3500]\tvalid_0's l1: 6.18936\n[3600]\tvalid_0's l1: 6.18599\n[3700]\tvalid_0's l1: 6.1837\n[3800]\tvalid_0's l1: 6.18128\n[3900]\tvalid_0's l1: 6.17875\n[4000]\tvalid_0's l1: 6.17632\n[4100]\tvalid_0's l1: 6.17391\n[4200]\tvalid_0's l1: 6.1715\n[4300]\tvalid_0's l1: 6.1687\n[4400]\tvalid_0's l1: 6.16611\n[4500]\tvalid_0's l1: 6.16341\n[4600]\tvalid_0's l1: 6.16059\n[4700]\tvalid_0's l1: 6.15781\n[4800]\tvalid_0's l1: 6.15577\n[4900]\tvalid_0's l1: 6.15215\n[5000]\tvalid_0's l1: 6.14831\nDid not meet early stopping. Best iteration is:\n[5000]\tvalid_0's l1: 6.14831\nModel for fold 2 saved to modelitos_para_despues/doblecito_2.txt\nFold 2 MAE: 6.148311302872351\nFold 3 Model Training\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 6.33686\n[200]\tvalid_0's l1: 6.30249\n[300]\tvalid_0's l1: 6.28095\n[400]\tvalid_0's l1: 6.26221\n[500]\tvalid_0's l1: 6.24702\n[600]\tvalid_0's l1: 6.2337\n[700]\tvalid_0's l1: 6.22134\n[800]\tvalid_0's l1: 6.21021\n[900]\tvalid_0's l1: 6.20011\n[1000]\tvalid_0's l1: 6.19139\n[1100]\tvalid_0's l1: 6.18313\n[1200]\tvalid_0's l1: 6.1757\n[1300]\tvalid_0's l1: 6.16849\n[1400]\tvalid_0's l1: 6.16115\n[1500]\tvalid_0's l1: 6.15451\n[1600]\tvalid_0's l1: 6.14749\n[1700]\tvalid_0's l1: 6.14079\n[1800]\tvalid_0's l1: 6.13433\n[1900]\tvalid_0's l1: 6.12864\n[2000]\tvalid_0's l1: 6.12342\n[2100]\tvalid_0's l1: 6.11794\n[2200]\tvalid_0's l1: 6.11164\n[2300]\tvalid_0's l1: 6.10632\n[2400]\tvalid_0's l1: 6.10094\n[2500]\tvalid_0's l1: 6.09552\n[2600]\tvalid_0's l1: 6.09014\n[2700]\tvalid_0's l1: 6.08457\n[2800]\tvalid_0's l1: 6.07964\n[2900]\tvalid_0's l1: 6.07519\n[3000]\tvalid_0's l1: 6.0713\n[3100]\tvalid_0's l1: 6.06718\n[3200]\tvalid_0's l1: 6.06314\n[3300]\tvalid_0's l1: 6.05912\n[3400]\tvalid_0's l1: 6.05536\n[3500]\tvalid_0's l1: 6.05169\n[3600]\tvalid_0's l1: 6.04803\n[3700]\tvalid_0's l1: 6.04501\n[3800]\tvalid_0's l1: 6.03967\n[3900]\tvalid_0's l1: 6.03554\n[4000]\tvalid_0's l1: 6.03166\n[4100]\tvalid_0's l1: 6.02732\n[4200]\tvalid_0's l1: 6.0233\n[4300]\tvalid_0's l1: 6.0192\n[4400]\tvalid_0's l1: 6.01588\n[4500]\tvalid_0's l1: 6.01188\n[4600]\tvalid_0's l1: 6.00812\n[4700]\tvalid_0's l1: 6.00419\n[4800]\tvalid_0's l1: 6.00091\n[4900]\tvalid_0's l1: 5.99809\n[5000]\tvalid_0's l1: 5.99519\nDid not meet early stopping. Best iteration is:\n[5000]\tvalid_0's l1: 5.99519\nModel for fold 3 saved to modelitos_para_despues/doblecito_3.txt\nFold 3 MAE: 5.995192353671792\nFold 4 Model Training\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 5.97582\n[200]\tvalid_0's l1: 5.9386\n[300]\tvalid_0's l1: 5.91263\n[400]\tvalid_0's l1: 5.88929\n[500]\tvalid_0's l1: 5.87046\n[600]\tvalid_0's l1: 5.85331\n[700]\tvalid_0's l1: 5.83754\n[800]\tvalid_0's l1: 5.82378\n[900]\tvalid_0's l1: 5.81118\n[1000]\tvalid_0's l1: 5.79994\n[1100]\tvalid_0's l1: 5.78953\n[1200]\tvalid_0's l1: 5.77891\n[1300]\tvalid_0's l1: 5.76876\n[1400]\tvalid_0's l1: 5.76034\n[1500]\tvalid_0's l1: 5.75151\n[1600]\tvalid_0's l1: 5.74247\n[1700]\tvalid_0's l1: 5.73292\n[1800]\tvalid_0's l1: 5.72425\n[1900]\tvalid_0's l1: 5.71597\n[2000]\tvalid_0's l1: 5.70766\n[2100]\tvalid_0's l1: 5.69989\n[2200]\tvalid_0's l1: 5.69163\n[2300]\tvalid_0's l1: 5.6835\n[2400]\tvalid_0's l1: 5.67515\n[2500]\tvalid_0's l1: 5.66737\n[2600]\tvalid_0's l1: 5.66006\n[2700]\tvalid_0's l1: 5.65344\n[2800]\tvalid_0's l1: 5.6468\n[2900]\tvalid_0's l1: 5.64079\n[3000]\tvalid_0's l1: 5.63481\n[3100]\tvalid_0's l1: 5.62924\n[3200]\tvalid_0's l1: 5.62183\n[3300]\tvalid_0's l1: 5.61489\n[3400]\tvalid_0's l1: 5.60914\n[3500]\tvalid_0's l1: 5.60359\n[3600]\tvalid_0's l1: 5.59782\n[3700]\tvalid_0's l1: 5.59105\n[3800]\tvalid_0's l1: 5.58429\n[3900]\tvalid_0's l1: 5.579\n[4000]\tvalid_0's l1: 5.57389\n[4100]\tvalid_0's l1: 5.56972\n[4200]\tvalid_0's l1: 5.56476\n[4300]\tvalid_0's l1: 5.55976\n[4400]\tvalid_0's l1: 5.55533\n[4500]\tvalid_0's l1: 5.55159\n[4600]\tvalid_0's l1: 5.54853\n[4700]\tvalid_0's l1: 5.54527\n[4800]\tvalid_0's l1: 5.54187\n[4900]\tvalid_0's l1: 5.53798\n[5000]\tvalid_0's l1: 5.53306\nDid not meet early stopping. Best iteration is:\n[5000]\tvalid_0's l1: 5.53306\nModel for fold 4 saved to modelitos_para_despues/doblecito_4.txt\nFold 4 MAE: 5.5330638598080055\nFold 5 Model Training\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 4.83998\n[200]\tvalid_0's l1: 4.82342\n[300]\tvalid_0's l1: 4.81648\n[400]\tvalid_0's l1: 4.8115\n[500]\tvalid_0's l1: 4.80923\n[600]\tvalid_0's l1: 4.80586\n[700]\tvalid_0's l1: 4.80355\n[800]\tvalid_0's l1: 4.80313\n[900]\tvalid_0's l1: 4.80199\nEarly stopping, best iteration is:\n[886]\tvalid_0's l1: 4.80194\nModel for fold 5 saved to modelitos_para_despues/doblecito_5.txt\nFold 5 MAE: 4.80193571943633\nTraining final model with average best iteration: 4177\nFinal model saved to modelitos_para_despues/doblez-conjunto.txt\nAverage MAE across all folds: 5.852327992741203\n","output_type":"stream"}]},{"cell_type":"code","source":"def zero_sum(prices, volumes):\n    std_error = np.sqrt(volumes)\n    step = np.sum(prices) / np.sum(std_error)\n    out = prices - std_error * step\n    return out\n\nif is_infer:\n    import optiver2023\n    env = optiver2023.make_env()\n    iter_test = env.iter_test()\n    counter = 0\n    y_min, y_max = -64, 64\n    qps, predictions = [], []\n    cache = pd.DataFrame()\n\n    # Weights for each fold model\n    model_weights = [1/len(models)] * len(models) \n    \n    for (test, revealed_targets, sample_prediction) in iter_test:\n        now_time = time.time()\n        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n        if counter > 0:\n            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n        feat = generate_all_features(cache)[-len(test):]\n\n        # Generate predictions for each model and calculate the weighted average\n        lgb_predictions = np.zeros(len(test))\n        for model, weight in zip(models, model_weights):\n            lgb_predictions += weight * model.predict(feat)\n\n        lgb_predictions = zero_sum(lgb_predictions, test['bid_size'] + test['ask_size'])\n        clipped_predictions = np.clip(lgb_predictions, y_min, y_max)\n        sample_prediction['target'] = clipped_predictions\n        env.predict(sample_prediction)\n        counter += 1\n        qps.append(time.time() - now_time)\n        if counter % 10 == 0:\n            print(counter, 'qps:', np.mean(qps))\n\n    time_cost = 1.146 * np.mean(qps)\n    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")\n","metadata":{"papermill":{"duration":52.710045,"end_time":"2023-10-31T03:55:58.288323","exception":false,"start_time":"2023-10-31T03:55:05.578278","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-11-07T07:13:53.383309Z","iopub.execute_input":"2023-11-07T07:13:53.383594Z","iopub.status.idle":"2023-11-07T07:15:54.512682Z","shell.execute_reply.started":"2023-11-07T07:13:53.383569Z","shell.execute_reply":"2023-11-07T07:15:54.511746Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"This version of the API is not optimized and should not be used to estimate the runtime of your code on the hidden test set.\n10 qps: 0.8029939889907837\n20 qps: 0.7569117307662964\n30 qps: 0.7443587621053059\n40 qps: 0.7394563615322113\n50 qps: 0.743266863822937\n60 qps: 0.7393448988596598\n70 qps: 0.7348496403012957\n80 qps: 0.731653270125389\n90 qps: 0.7300550248887804\n100 qps: 0.734042363166809\n110 qps: 0.7317776484922929\n120 qps: 0.730555651585261\n130 qps: 0.7290188404229971\n140 qps: 0.7310168181146894\n150 qps: 0.7299631182352702\n160 qps: 0.7289176613092423\nThe code will take approximately 0.835 hours to reason about\n","output_type":"stream"}]}]}